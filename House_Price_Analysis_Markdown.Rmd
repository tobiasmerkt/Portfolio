---
title: "House_Price_Analysis_Markdown"
author: "Tobias Merkt"
date: "5/15/2018"
output: html_document
---

The goal of this analysis is mostly to practice data cleaning. Using this cleaned data, a linear model is created to estimate the house prices.

# 1. Preparation
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "~/Desktop/IE/Semester 2/ML/Assignments/Assignment 1")
dyn.load('/Library/Java/JavaVirtualMachines/jdk-9.0.1.jdk/Contents/Home/lib/server/libjvm.dylib')
library(rJava)
library(dummies)
library(ggplot2)
library(plyr)
library(dplyr)
library(moments)
library(glmnet)
library(caret)
library(MASS)
library(FSelector)
library(stringr) 
library(Matrix) 
library(Metrics) 
library(scales)
library(corrplot)
library(ltm)             
library(pROC)            
library(ROCR)
library(GGally)
library(psych)
```

At first, the training and test dataframes are joined to do the data cleaning part. To join the two dataframes, they must have the same columns. Thus, the SalePrice column is created for the test dataset.
```{r load and combine datasets}
dataset <- read.csv("train.csv")
test_data <- read.csv("test.csv")
test_data$SalePrice <- 0
dataset <- rbind(dataset, test_data)
```


# 2. Null imputation

Now, the quality of the dataset is checked: Does it contain duplicates (i.e., multiple similar IDs)? After answering this question (no, it does not contain duplicates), the ID column is removed as it does not add any value to the analysis.
```{r load and combine datasets}
length(unique(dataset$Id)) == nrow(dataset)
dataset = dataset[ , -which(names(dataset) %in% c("Id"))]
```

There are many categories with missing values in this dataset; some of them are even almost completely missing (PoolQC is missing in 2909 out of 2919 instances).
```{r NAs discovery}
na.cols <- which(colSums(is.na(dataset)) > 0)
sort(colSums(sapply(dataset[na.cols], is.na)), decreasing = TRUE)
paste('There are', length(na.cols), 'columns with missing values')
```

Those columns will be dealt with one after another. For the pool column, there are three houses with positive PoolArea but no PoolQC (Quality). Due to the small number of houses, the missing values are imputed by hand by calculating the mean poolsize for all pools of a certain quality. The quality of all other NA's is imputed as None.

```{r PoolQC}
summary(dataset$PoolQC)
summary(dataset$PoolArea)
ggplot(dataset, aes(x=PoolArea)) +
  geom_histogram()
dataset[(dataset$PoolArea > 0) & is.na(dataset$PoolQC),c('PoolQC','PoolArea')]
dataset[,c('PoolQC','PoolArea')] %>%
  group_by(PoolQC) %>%
  summarise(mean = mean(PoolArea), counts = n()) 

dataset[2421,'PoolQC'] = 'Ex'
dataset[2504,'PoolQC'] = 'Ex'
dataset[2600,'PoolQC'] = 'Fa'

dataset$PoolQC = factor(dataset$PoolQC, levels=c(levels(dataset$PoolQC), "None"))
dataset$PoolQC[is.na(dataset$PoolQC)] = 'None'
```

For the following columns, NA simply means "None" or O:
```{r None imputation}
bsmt.cols <- names(dataset)[sapply(names(dataset), function(x) str_detect(x, 'Bsmt'))]

for (col in bsmt.cols) {
  dataset[,col] = factor(dataset[,col], levels=c(levels(dataset[,col]), "None"))
}

for (col in bsmt.cols){
  if (sapply(dataset[col], is.numeric) == TRUE){
    dataset[sapply(dataset[col], is.na),col] = 0
  }
  else{
    dataset[sapply(dataset[col],is.na),col] = 'None'
  }
}

garage.cols <- c('GarageArea', 'GarageCars', 'GarageQual', 'GarageFinish', 'GarageCond', 'GarageType')


for (col in garage.cols){
  dataset[,col] = factor(dataset[,col], levels = c(levels(dataset[,col]), "None"))
}

for (col in garage.cols){
  if (sapply(dataset[col], is.numeric) == TRUE){
    dataset[sapply(dataset[col], is.na), col] = 0
  }
  else{
    dataset[sapply(dataset[col], is.na), col] = 'None'
  }
}

# Other columns
null_cols <- c('Alley', 'Fence', 'FireplaceQu', 'MiscFeature')
for (col in null_cols) {
  dataset[,col] = factor(dataset[,col], levels=c(levels(dataset[,col]), "None"))
  dataset[is.na(dataset[,col]), col] = "None"
}

dataset$MasVnrArea[is.na(dataset$MasVnrArea)] <- 0

## MasVnrType : None level exists already
dataset$MasVnrType[is.na(dataset$MasVnrType)] = "None"
```


GarageYrBlt: Most likely, the garages were simultaneously to the houses.
``` {r Garage Year}
garage_year_miss <- which(is.na(dataset$GarageYrBlt))
dataset[garage_year_miss, 'GarageYrBlt'] <- dataset[garage_year_miss, 'YearBuilt']
```


Obviously, the lot size depends on the neighbourhood to a large extent. Thus, the NA values are replaced by the median Lot Frontage values in the respective neighbourhoods.
``` {r Lot Frontage}
dataset['Nbrh.factor'] <- factor(dataset$Neighborhood, levels = unique(dataset$Neighborhood))
factor(dataset$Neighborhood, levels = unique(dataset$Neighborhood))

lot.by.nbrh <- dataset[,c('Neighborhood','LotFrontage')] %>%
  group_by(Neighborhood) %>%
  summarise(median = median(LotFrontage, na.rm = TRUE))
lot.by.nbrh

na_lotfrontage = which(is.na(dataset$LotFrontage))

for (i in na_lotfrontage){
  lot.median <- lot.by.nbrh[lot.by.nbrh$Neighborhood == dataset$Neighborhood[i],'median']
  dataset[i,'LotFrontage'] <- lot.median[[1]]
}
```


As can be seen, the functional column is, by overwhelming majority, "Typ" and thus imputed accordingly.
``` {r Functional}
qplot(dataset$Functional)
dataset$Functional[is.na(dataset$Functional)] = 'Typ'
```

Since only one value of Kitchen Quality is missing, this value will just be assigned to the class with most counts: "TA"
``` {r Kitchen Quality}
qplot(dataset$KitchenQual)
sum(is.na(dataset$KitchenQual))
dataset$KitchenQual[is.na(dataset$KitchenQual)] = 'TA'
```

Again, only one value is missing and most values are of type "SBrkr".
``` {r Electrical}
qplot(dataset$Electrical)
sum(is.na(dataset$Electrical))
dataset$Electrical[is.na(dataset$Electrical)] = 'SBrkr'
```


Similar to the previous categories, the missing values of the following four categories will be imputed with the mode value:
``` {r Remaining missing values}
dataset$MSZoning[is.na(dataset$MSZoning)] <- names(sort(-table(dataset$MSZoning)))[1]
dataset$SaleType[is.na(dataset$SaleType)] <- names(sort(-table(dataset$SaleType)))[1]
dataset$Exterior1st[is.na(dataset$Exterior1st)] <- names(sort(-table(dataset$Exterior1st)))[1]
dataset$Exterior2nd[is.na(dataset$Exterior2nd)] <- names(sort(-table(dataset$Exterior2nd)))[1]
```

# Utilities and MasVnrArea missing
The utilities column has only one value other than "AllPub". It is not useful to include it in the modeling as the sample size is not sufficiently large and overfitting is likely.
``` {r Missing values check}
summary(dataset$Utilities)
dataset <- dataset[,-which(names(dataset) == "Utilities")]
```

``` {r Missing values check}
na.cols <- which(colSums(is.na(dataset)) > 0)
paste('There are now', length(na.cols), 'columns with missing values')
```


# 3. Outliers

For a linear model, it is crucial to deal with outlier values as they disproportionally influence the model. TO spot these outliers, the numerical variables will be plotted against the sales price.


As can be seen in the graph below, there seems to be a positive correlation between SalePrice and the gross living area. Some houses, however, have very large living areas yet sell relatively cheap. For those houses with a gross living area > 4000, the GrLivArea is changed to the mean GrLivArea For very important projects, one could also train a model which estimates the gross living area for the outliers.

``` {r Outlier - GrLivArea}
ggplot(dataset[which(dataset$SalePrice > 0),], aes(x=GrLivArea, y=SalePrice)) +
  geom_point() +
  geom_smooth(mapping = aes(linetype = "r2"),
              method = "lm",
              formula = y ~ x + log(x), se = FALSE,
              color = "red")

dataset$GrLivArea[dataset$GrLivArea>4000] <- mean(dataset$GrLivArea)%>%as.numeric
```

As can be seen in the graph below, the relationship between the LotArea and the SalePrice is heavily influenced by a few outliers. For those houses with a LotArea > 50,000, the data is changed to the mean of the variable.

``` {r Outlier - LotArea}
ggplot(dataset[which(dataset$SalePrice > 0),], aes(x=LotArea, y=SalePrice)) +
  geom_point() +
  geom_smooth(mapping = aes(linetype = "r2"),
              method = "lm",
              formula = y ~ x + log(x), se = FALSE,
              color = "red")
dataset$LotArea[dataset$LotArea>50000] <- mean(dataset$LotArea)%>%as.numeric
```

In the X1stFlrSF variable, only few instances are clear outliers. To improve the model, the value of those instances with X1stFlrSF > 2,800 will be set to the mean
``` {r Outlier - X1stFlrSF}
ggplot(dataset[which(dataset$SalePrice > 0),], aes(x=X1stFlrSF, y=SalePrice)) +
  geom_point() +
  geom_smooth(mapping = aes(linetype = "r2"),
              method = "lm",
              formula = y ~ x + log(x), se = FALSE,
              color = "red")
dataset$X1stFlrSF[dataset$X1stFlrSF>2800] <- mean(dataset$X1stFlrSF)%>%as.numeric
```

This variable has no clear outliers and hence no data modification is needed. 
``` {r Outlier - X2ndFlrSF}
ggplot(dataset[which(dataset$SalePrice > 0),], aes(x=X2ndFlrSF, y=SalePrice)) +
  geom_point()
```


Again, no outliers need to be modified.
``` {r Outlier - LowQualFinSF}
ggplot(dataset[which(dataset$SalePrice > 0),], aes(x=LowQualFinSF, y=SalePrice)) +
  geom_point()
```

Only three outliers were changed to the mean value; however, MiscVal anyways seems not to be strongly correlated with SalePrice.
``` {r Outlier - MiscVal}
ggplot(dataset[which(dataset$SalePrice > 0),], aes(x=MiscVal, y=SalePrice)) +
  geom_point()
dataset$MiscVal[dataset$MiscVal>2500] <- mean(dataset$MiscVal)%>%as.numeric
```


# 4. Feature Recoding/ Factorisation


## 4.1. Transform character variables into numerical variables

Some variables represent a ranking (e.g., "Excellent", "Fair", "Good", ...). With a linear model, this ranking can not be displayed correctly if one does not change the variable type to numeric. 

```{r Character values into numerical factors}
dataset$PoolQC<- recode(dataset$PoolQC,"None"=0,"Po"=1,"Fa"=2,"TA"=3,"Gd"=4,"Ex"=5)
dataset$Fence<- recode(dataset$Fence,"None"=0,"MnWw"=1,"GdWo"=2,"MnPrv"=3,"GdPrv"=4)
dataset$ExterQual<- recode(dataset$ExterQual,"None"=0,"Po"=1,"Fa"=2,"TA"=3,"Gd"=4,"Ex"=6)
dataset$ExterCond<- recode(dataset$ExterCond,"None"=0,"Po"=1,"Fa"=2,"TA"=3,"Gd"=4,"Ex"=6)
dataset$BsmtQual<- recode(dataset$BsmtQual,"None"=0,"Po"=1,"Fa"=2,"TA"=3,"Gd"=4,"Ex"=5)
dataset$BsmtCond<- recode(dataset$BsmtCond,"None"=0,"Po"=1,"Fa"=2,"TA"=3,"Gd"=4,"Ex"=5)
dataset$BsmtExposure<- recode(dataset$BsmtExposure,"None"=0,"No"=1,"Mn"=2,"Av"=3,"Gd"=4)
dataset$BsmtFinType1<- recode(dataset$BsmtFinType1,"None"=0,"Unf"=1,"LwQ"=2,"Rec"=3,"BLQ"=4,"ALQ"=5,"GLQ"=6)
dataset$BsmtFinType2<- recode(dataset$BsmtFinType2,"None"=0,"Unf"=1,"LwQ"=2,"Rec"=3,"BLQ"=4,"ALQ"=5,"GLQ"=6)
dataset$GarageFinish<- recode(dataset$GarageFinish,"None"=0,"Unf"=1,"RFn"=2,"Fin"=3)
dataset$GarageQual<- recode(dataset$GarageQual,"None"=0,"Po"=1,"Fa"=2,"TA"=3,"Gd"=4,"Ex"=5)
dataset$GarageCond<- recode(dataset$GarageCond,"None"=0,"Po"=1,"Fa"=2,"TA"=3,"Gd"=4,"Ex"=5)
dataset$HeatingQC<- recode(dataset$HeatingQC,"None"=0,"Po"=1,"Fa"=2,"TA"=3,"Gd"=4,"Ex"=5)
dataset$KitchenQual<- recode(dataset$KitchenQual,"None"=0,"Po"=1,"Fa"=2,"TA"=3,"Gd"=4,"Ex"=5)
dataset$Functional<- recode(dataset$Functional,"None"=0,"Sev"=1,"Maj2"=2,"Maj1"=3,"Mod"=4,"Min2"=5,"Min1"=6,"Typ"=7)
dataset$FireplaceQu<- recode(dataset$FireplaceQu,"None"=0,"Po"=1,"Fa"=2,"TA"=3,"Gd"=4,"Ex"=5)
```
With sufficient domain knowledge, one could also create a binarised version of the ranking (E.g., Quality = Poor if <3 and Quality = Good if >=3).


## 4.2. Neighbourhood value


```{r Neighbourhood}
dataset[,c('Neighborhood','SalePrice')] %>%
  filter(SalePrice > 0) %>%
  group_by(Neighborhood) %>%
  summarise(median.price = median(SalePrice, na.rm = TRUE)) %>%
  arrange(median.price) %>%
  mutate(nhbr.sorted = factor(Neighborhood, levels=Neighborhood)) %>%
  ggplot(aes(x=nhbr.sorted, y=median.price)) +
  geom_point() +
  geom_text(aes(label = median.price, angle = 45), vjust = 2) +
  theme_minimal() +
  labs(x='Neighborhood', y='Median price') +
  theme(text = element_text(size=12),
        axis.text.x = element_text(angle=45))
```

Obviously, the sales price is higher in some neighbourhoods than in others. To take this into account, the Neighbourhood category will be transformed into a ranked numerical value (0=Lowest SalePrice, 24 = Highest SalePrice). Also, the neighbourhoods seem to have a relatively linear correlation with SalePrice except for three neighbourhoods (StoneBr, NoRidge, NridgHt). To create a better linear model, a binarised feature will be created for those three neighbourhoods.

```{r Neighbourhood Impact}
nbrh.rich <- c('StoneBr', 'NoRidge', 'NridgeHt')
dataset['NbrhRich'] <- (dataset$Neighborhood %in% nbrh.rich) *1

dataset$Neighborhood <- recode(dataset$Neighborhood, 'MeadowV' = 0, 'IDOTRR' = 1, 'BrDale' = 2, 'OldTown' = 3, 'Edwards' = 4, 'BrkSide' = 5, 'Sawyer' = 6, 'Blueste' = 7, 'SWISU' = 8, 'NAmes' = 9, 'NPkVill' = 10, 'Mitchel' = 11, 'SawyerW' = 12, 'Gilbert' = 13, 'NWAmes' = 14, 'Blmngtn' = 15, 'CollgCr' = 16, 'ClearCr' = 17, 'Crawfor' = 18, 'Veenker' = 19, 'Somerst' = 20, 'Timber' = 21, 'StoneBr' = 22, 'NoRidge' = 23, 'NridgHt' = 24)
```


## 4.3. Transform numerical features to categorical

Some features such as Month should be transformed to categorical since they are not numerical.

```{r Factorize features}
dataset$MSSubClass <- as.factor(dataset$MSSubClass)
dataset$YrSold <- as.factor(dataset$YrSold)
dataset$MoSold <- as.factor(dataset$MoSold)
```

# 5. Feature Creation

By collaborating with domain experts, one can find endless combinations of features which improve the model. For demonstration purposes, I've included two which sound reasonable to someone without any expertise in the American real estate market:
1) TimeRemod = The time spent since the last remodelling of the house and the year when it was sold.
2) NewBuilt = Binary column which classifies whether houses were sold in the same year as they were build (which sounds reasonable to increase SalePrice).

```{r Feature Creation}
dataset$TimeRemod <- as.numeric(dataset$YrSold) - as.numeric(dataset$YearRemodAdd)
dataset$NewBuilt <- (dataset$YearBuilt == dataset$YrSold) * 1
```

A linear model sometimes works better when the most important variables are also represented on a non-linear scale. Thus, the most important variables are squared to capture this relationship (one could additionally set it to the power of three or higher or also calculate the squareroot or logarithm if one expects a nonlinear relationship which can not adequately be captured by squared variables).

```{r Nonlinear Relationships}
num_features <- names(which(sapply(dataset, is.numeric)))
num_features_set <- dataset[num_features]

correlations <- cor(num_features_set)
corr.SalePrice <- as.matrix(sort(correlations[,'SalePrice'], decreasing = TRUE))
strong.corr.SalePrice <- names(which(apply(corr.SalePrice, 1, function(x) (x > 0.23 | x < -0.23))))
```

```{r Squared functions}
dataset["OverallQual-2"] <- sapply(dataset$OverallQual, function(x){x**2})
dataset["GrLivArea-2"] = sapply(dataset$GrLivArea, function(x){x**2})
dataset["ExterQual-2"] = sapply(dataset$ExterQual, function(x){x**2})
dataset["KitchenQual-2"] = sapply(dataset$KitchenQual, function(x){x**2})
dataset["TotRmsAbvGrd-2"] <- sapply(dataset$TotRmsAbvGrd, function(x){x**2})
dataset["X1stFlrSF-2"] = sapply(dataset$X1stFlrSF, function(x){x**2})
```


# 6. Data preparation

The model can learn better if categorical features are dummified:

```{r Dummification}
dataset <-dummy.data.frame(dataset,dummy.classes = "factor")
```

A linear model requires a normal distribution. To decrease the skewness, a log transformation is used.

```{r Log SalePrice}
# Log transform the target for official scoring
dataset$SalePrice <- log1p(dataset$SalePrice)

column_types <- sapply(names(dataset),function(x){class(dataset[[x]])})
numeric_columns <-names(column_types[column_types != "character"])
skew <- sapply(numeric_columns,function(x){skewness(dataset[[x]],na.rm = T)})
skew <- skew[skew > 0.8 | skew < -0.8] 
for (x in names(skew)) {
  dataset[[x]] <- log(dataset[[x]] + 1) # +1 since log(0) =  infinity
}
```


```{r Train test split}
final_train <- dataset[1:1460,]
final_test <- dataset[1461:2919,]
```


# 7. Feature Selection using Lasso

```{r Lasso Regression, warning=FALSE}
set.seed(42)
lasso <- cv.glmnet(x = data.matrix(final_train[, - which(names(final_train) %in% c('SalePrice'))]), y = final_train$SalePrice, nfolds = 10)
plot(lasso)
```

As seen in the figure, lambda min is close to 0. In particular it is equal to:
```{r}
lasso$lambda.min
```

Cross-Validated error for lambda min (RMSE)
```{r}
sqrt(lasso$cvm[lasso$lambda == lasso$lambda.min])
```


# 8. Final Submission


```{r Final Submission}
set.seed(42)
lasso_pred <- as.numeric(exp(predict(lasso, newx = data.matrix(final_test[, - which(names(final_test) %in% c('SalePrice'))]), s = "lambda.min"))-1) #Exp because we calculated the log of SalePrice

lasso_submission <- data.frame(Id = test_data$Id, SalePrice= (lasso_pred))
colnames(lasso_submission) <-c("Id", "SalePrice")
write.csv(lasso_submission, file = "final_submission.csv", row.names = FALSE) 
```




